{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "리뷰는나오는데3곳만됨.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgPdHPFmqKC94o+09qKSkd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabbi120101/TIL/blob/master/ML/2%EC%B0%A8%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%A6%AC%EB%B7%B0%EB%8A%94%EB%82%98%EC%98%A4%EB%8A%94%EB%8D%B03%EA%B3%B3%EB%A7%8C%EB%90%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BXbSQdg5xSO"
      },
      "source": [
        "# pip install selenium"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWZ50zQb6ALW",
        "outputId": "8049f7e7-eb18-4614-9cba-2ad9f79f29f5"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !apt-get update\n",
        "    !apt install chromium-chromedriver\n",
        "    !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "    !pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 31.5 kB/88.7\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r                                                                               \r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                         \rHit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (108 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (91.0.4472.101-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 108 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XqYt_vDu5vlM",
        "outputId": "e4c0f957-f818-4da9-c20e-1ec6ad959d68"
      },
      "source": [
        "import os\n",
        "from time import sleep\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.common.exceptions import ElementNotInteractableException\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "##############################################################  ############\n",
        "##################### variable related selenium ##########################\n",
        "##########################################################################\n",
        "# options = webdriver.ChromeOptions()\n",
        "# 크롬창(웹드라이버) 열기\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "# 구글 지도 접속하기\n",
        "driver.get(\"https://www.google.com/maps/\")\n",
        "\n",
        "# 검색창에 \"카페\" 입력하기\n",
        "searchbox = driver.find_element_by_css_selector(\"input#searchboxinput\")\n",
        "searchbox.send_keys(\"place\")\n",
        "\n",
        "# 검색버튼 누르기\n",
        "searchbutton = driver.find_element_by_css_selector(\"button#searchbox-searchbutton\")\n",
        "searchbutton.click()\n",
        "\n",
        "def main():\n",
        "    global driver, load_wb, review_num\n",
        "\n",
        "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
        "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
        "\n",
        "    # 검색할 목록\n",
        "    place_infos = ['신림 카페']\n",
        "\n",
        "    for i, place in enumerate(place_infos):\n",
        "        # delay\n",
        "        if i % 4 == 0 and i != 0:\n",
        "            sleep(5)\n",
        "        print(\"#####\", i)\n",
        "        search(place)\n",
        "\n",
        "    driver.quit()\n",
        "    print(\"finish\")\n",
        "\n",
        "\n",
        "def search(place):\n",
        "    global driver\n",
        "\n",
        "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
        "    search_area.send_keys(place)  # 검색어 입력\n",
        "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
        "    sleep(1)\n",
        "\n",
        "    # 검색된 정보가 있는 경우에만 탐색\n",
        "    # 1번 페이지 place list 읽기\n",
        "    html = driver.page_source\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
        "\n",
        "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
        "    crawling(place, place_lists)\n",
        "    search_area.clear()\n",
        "\n",
        "    # 우선 더보기 클릭해서 2페이지\n",
        "    try:\n",
        "        driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
        "        sleep(1)\n",
        "\n",
        "        # 2~ 5페이지 읽기\n",
        "        for i in range(2, 6):\n",
        "            # 페이지 넘기기\n",
        "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
        "            driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
        "            sleep(1)\n",
        "\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
        "\n",
        "            crawling(place, place_lists)\n",
        "\n",
        "    except ElementNotInteractableException:\n",
        "        print('not found')\n",
        "    finally:\n",
        "        search_area.clear()\n",
        "\n",
        "\n",
        "def crawling(place, place_lists):\n",
        "    \"\"\"\n",
        "    페이지 목록을 받아서 크롤링 하는 함수\n",
        "    :param place: 리뷰 정보 찾을 장소이름\n",
        "    \"\"\"\n",
        "\n",
        "    while_flag = False\n",
        "    for i, place in enumerate(place_lists):\n",
        "        # 광고에 따라서 index 조정해야함\n",
        "        #if i >= 3:\n",
        "         #   i += 1\n",
        "\n",
        "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
        "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
        "\n",
        "        detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
        "        driver.find_element_by_xpath(detail_page_xpath).send_keys(Keys.ENTER)\n",
        "        driver.switch_to.window(driver.window_handles[-1])  # 상세정보 탭으로 변환\n",
        "        sleep(1)\n",
        "\n",
        "        print('####', place_name)\n",
        "\n",
        "        # 첫 페이지\n",
        "        extract_review(place_name)\n",
        "\n",
        "        # 2-5 페이지\n",
        "        idx = 3\n",
        "        try:\n",
        "            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
        "            for i in range(page_num-1):\n",
        "                # css selector를 이용해 페이지 버튼 누르기\n",
        "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
        "                sleep(1)\n",
        "                extract_review(place_name)\n",
        "                idx += 1\n",
        "            driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 5페이지가 넘는 경우 다음 버튼 누르기\n",
        "            sleep(1)\n",
        "            extract_review(place_name) # 리뷰 추출\n",
        "        except (NoSuchElementException, ElementNotInteractableException):\n",
        "            print(\"no review in crawling\")\n",
        "\n",
        "        # 그 이후 페이지\n",
        "        while True:\n",
        "            idx = 4\n",
        "            try:\n",
        "                page_num = len(driver.find_elements_by_class_name('link_page'))\n",
        "                for i in range(page_num-1):\n",
        "                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
        "                    sleep(1)\n",
        "                    extract_review(place_name)\n",
        "                    idx += 1\n",
        "                driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 10페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
        "                sleep(1)\n",
        "                extract_review(place_name) # 리뷰 추출\n",
        "            except (NoSuchElementException, ElementNotInteractableException):\n",
        "                print(\"no review in crawling\")\n",
        "                break\n",
        "\n",
        "        driver.close()\n",
        "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
        "\n",
        "\n",
        "def extract_review(place_name):\n",
        "    global driver\n",
        "\n",
        "    ret = True\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # 첫 페이지 리뷰 목록 찾기\n",
        "    review_lists = soup.select('.list_evaluation > li')\n",
        "\n",
        "    # 리뷰가 있는 경우\n",
        "    if len(review_lists) != 0:\n",
        "        for i, review in enumerate(review_lists):\n",
        "            comment = review.select('.txt_comment > span') # 리뷰\n",
        "            rating = review.select('.grade_star > em') # 별점\n",
        "            val = ''\n",
        "            if len(comment) != 0:\n",
        "                if len(rating) != 0:\n",
        "                    val = comment[0].text + '/' + rating[0].text.replace('점', '')\n",
        "                else:\n",
        "                    val = comment[0].text + '/0'\n",
        "                print(val)\n",
        "\n",
        "    else:\n",
        "        print('no review in extract')\n",
        "        ret = False\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### 0\n",
            "#### 디자이너리카페\n",
            "no review in extract\n",
            "환하고 예쁨. ㅁ자형 구조라 가운데 트인 부분이 기분을 상쾌하게 해줌. 커피맛은 잘 모르므로 이 부분에 대한 언급은 패스. 앉아서 노트북하고 공부하기 좋은 테이블, 의자였음. 재방문의사 있음./5\n",
            "매장 자체는 예쁜데 맛은 없어서 항상 재방문이 망설여짐/3\n",
            "넘 예쁘고 음료, 디저트도 맛나용/5\n",
            "장소가 주는 힙함. 아늑함. /4\n",
            "루프탑 풍경도 괜찮고 모던 인테리어치고 편안한 느낌. 햇빛이 잘 드는 지 화이트톤이라 그런건지 구름낀 날씨에도 눈부시고 맑아서 책 보기에 좋았음 친구들이랑 수다떨러 오고 싶은 곳. 근데 커피가 진짜 맛없음/3\n",
            "주말에 자리 진짜 없음.. 햇볕 강한 날 가면 직사광선에 온 몸 타는 기분을 느낄 수 있음 그리고 루프탑과 연결된 중정형?? 이라 그런지 더 시끄러운 것 같아어요. 도떼기시장 저리가라/3\n",
            "이쁜데 맛은 없음/3\n",
            "시설이랑 이런건 좋은데... 서비스는 조금 아쉽네요~! 조금더 좋게 해주셨으면 좋겠어요~/4\n",
            "/2\n",
            "/5\n",
            "분위기에 취한다/4\n",
            "자리값 커피는 모르겠다/3\n",
            "이 동네에 이런 카페가...? 와우! 믿기지않는다! 깔끔하고 넓은 내부, 독특한 인테리어에 루프탑까지 완벽- 다만, 커피맛이 좀 별로다. 여기 올거면 아아메 먹던지 차라리 병음료 추천 /4\n",
            "음료 진짜 맛없어요 자리 없는건 그러려니 하는데 시럽덩어리에 물 푼 맛임/1\n",
            "/4\n",
            "카페 인테리어는 정말 이쁘고 쾌적해서 자주오고 싶을것 같아요   그런데 커피양이 좀 적은것 같은건 기분탓이려나요 ㅠㅠ/3\n",
            "아메리카노 시켰는데 원두 다타서 두입먹고 다버림 뭐임이거 탄맛 에바잖어 ㅇ이거 다먹으면 암걸릴듯/1\n",
            "커피맛은 평범? 불만갖거나 감탄할 맛이 아니었고 디저트류도 지극히 평범 케잌은 떼오는것 같고 토스트는 빵 생크림 사과잼 모두가 평범합니다 의자도 딱딱하고 불편하고요 그렇지만 이곳을 먹으러 오는 곳이 아니라 널찍한 공간을 즐기러 오는 곳으로 생각한다면 신림에서 가장 만족스러운 공간일것입니다!/4\n",
            "카페는 예쁘지만 찾아가는 길 멀고 의자 불편함 재방문의사 없음/2\n",
            "신림에서 제일 힙한곳/4\n",
            "카페는 예쁜데 커피는 그냥그러네요. 디저트류는 비싼것같아요. /3\n",
            "의자 불편, 커피 적당/3\n",
            "영업시간을 고정할 필요가 있음 몇명이 헛걸음을 하는지 모든 사람들이 인스타만 들여다보고 살지는 않음/2\n",
            "엄청 힙한 가게가 신림에 생겼네요!/5\n",
            "새로 생긴지 얼마 안된 깔끔하고 예쁜 카페, 창이 크게 나서 햇빛도 잘 들고 커피도 맛있어요! 직원분들도 친절하네요 ㅎㅎㅎ/5\n",
            "이쁘고 커피맛도 좋고!! 사장님도 친절해여! /5\n",
            "no review in crawling\n",
            "#### 사생활\n",
            "no review in extract\n",
            "가보고 싶었던 만큼 맛있게 잘 먹고 왔으나 직원분 초큼 ㅎㅎ/4\n",
            "/5\n",
            "낮에 가는게 분위기 더좋아요 인테리어이쁨/5\n",
            "최악 불친절하고 시끄럽고 노래선곡 센스없음 음식도 가격대비 너무 비쌈 그나마 양이 적어서 맛없는거 겨우 먹고 도망치둣 나옴 /1\n",
            "파스타도 맛있고 돈가스도 맛있고 크로플까지 맛있음! 여기는 주기적으로 먹음..ㅠㅠ/5\n",
            "시골오일파스타가 정말 맛있어요 :)/5\n",
            "/4\n",
            "너무 추워서 꼬냑 한잔 마시러 들어갔는데 잔을 데워줬던 감성있는곳  /5\n",
            "메뉴에 없는 칵테일 주문했는데 세상제일 맛있었어요.. ^^b/5\n",
            "Amazing. Thank you/5\n",
            "양도 별로고 맛도 없고 원수 친구가 간다해도 말릴거같아요. 그리고 컵이 약간 지저분?해서 기분나빴어요/1\n",
            "세트로 시켰는데 음식이랑 음료가 같이 안 나옴...  목메여서 물마시면서 먹음. 음식 2/3 다먹어가는 시점에 음료가 나오더라구요...  결국엔 테이크아웃으로 가져감...  근데 더 늦게 오고 더 늦게 주문한 팀이 음식 음료 모두 먼저 받더군요 /1\n",
            "미팅있어서 근처에 외근갔다가, 너무 지쳐서  달달한거 먹으면서 위로받고 싶었다.   검색하다가 우연히 얻어 걸린 곳!  지하에 있는데 반층 내려가니 까페의 아이덴티티를 보여주듯 아기작한 입구가 보였다.   지하 내부는 쾌적했고, 조명 은은하고 깔끔.  가운데 큰 테이블이 있어서 노마드로 작업하기도 편해보였음.   곧 저녁 시간이라서 완전 배부른건 안되고...../5\n",
            "주택 지하1층에 있는 밥집 겸 카페 겸 술집. 분위기 좋습니다. 손님들 구성에 따라 너무 시끄럽다는 후기도 있었는데 제가 갔을때는 딱히 그렇지는 않았습니다. 읽을 만한 책과 잡지들이 있어 혼밥하고 쉬기에도 적합합니다. 주문한 메뉴는 닭가슴살 보울+음료 선택(아메리카노) 12,000원입니다. 생각보다 푸짐하게 나왔네요.  + 재방문 평일 저녁 늦게 갔는데 거.../5\n",
            "신대방역에 내리는 유일한 이유/5\n",
            "직원 분 짱 친절하시구 분위기도 맘에 들어요 낮에는 카페라던데 낮에도 가 보고 싶네요/5\n",
            "스카치에그랑 카레 정말 맛있는데, 고객수에 비해 일하는 사람 너무 적어서 메뉴 나오는데 한참걸리고 제가 방문한 날은 안되는 음료도 너무많았음. 장사가 잘되면 잘되는 만큼 사람을 더 고용하시는게 맞지않을까요/4\n",
            "이전에 조용하고 좋았는데 분위기가 바뀌고 나서  지금은 혼자서 노트북 영화보기에도 노래소리가 너무 커요 ㅠㅠㅠ 집중해서 노트북으로 작업하러 가시는분이나 가볍게 영상보러 가기에는 비추입니다 ㅠㅠ  분위기랑 노래는 좋으니 얘기나눌 분들은 가보셔도 좋을 것 같아요! /3\n",
            "커피 맥주와 함께 책을 읽을 수 있는 곳 조용한 분위기와 시그니처 음료, 음식들 정말 맛있어요 :) 가게가 구석구석 골목에 있어 찾기 힘들지만 가게에 들어가는 순간 다른 세계에 온 것 같은 느낌이 듭니다/5\n",
            "혼자만의 시간 갖기에 최적인 카페! 분위기 좋고 심지어 아메리카노도 아주 만족스러웠어요! 프릳츠 원두 사용하는거 같더라구요~ 다른 음료는 안먹어봐서 모르겠네요..ㅎㅎ/5\n",
            "사장님들이 갈 때마다 기억해주시고 착하셔서 기분이 좋슴다 분위기도 넘 좋구 라떼랑 술이 진짜 맛있어여 흘러 나오는 노래도 분위기랑 넘 잘 맞고 읽을 수 있게 구비된 책들도 너무 좋고요 20대에서 30대분들이 좋아할 카페예욤/5\n",
            "책 읽기 너무 좋아요 차분한 팝 음악도 마음에 들었고 치킨 파니니도 맛있었어요 양도 많고요 힐링되네요/5\n",
            "노래 대박이구 다 맛있음 .. /5\n",
            "분위기 깡패 커피 원두 장난아니네요.. 블루보틀 싱글오리진 섬머블랜드 사서 코박죽 할정도로 좋아하는데, 프릳츠... 메모.../5\n",
            "/4\n",
            "기본인 커피가 맛이없었다. 내가 간 시간만 그런건지 음악소리가 커서 아쉬웠음 클래식음악이 딱 좋을거같은 분위기인데/3\n",
            "/5\n",
            "no review in crawling\n",
            "#### 발루토\n",
            "no review in extract\n",
            "위생이랑 서비스 별로라고 적었는데 댓글 삭제하시나보네요 찔리긴 한 모양입니다/1\n",
            "경악했어요...어린이날 다음인가 직원이 일회용컵 닦아서 말리는거 봤어요... 그거보고 그냥 매장나옴./1\n",
            "신림동의 명물. 인생카페./5\n",
            "/5\n",
            "/4\n",
            "맛있지만 서비스가 영^^.. 바로 앞 계단에서 우당탕탕 넘어졌는데도 눈길도 안줌ㅋㅋ 키오스크도 이것보단 친절할듯..  옆에 계신 손님이 오히려 걱정해주고..(그분께 무한한 감사를••)/2\n",
            "/5\n",
            "스콘 맛집 커피는 쏘쏘  /4\n",
            "스콘은 맛있는데 대왕쿠키 대왕실망 ㅠㅜ /3\n",
            "맛잇는뎅?? 딸기라떼 딸기맛 지대루고 별로안담 아이스쿠림말차라떼엿나 그것두 진함  초코케익도 진하고 맛이쪔  +) 210108 재방문!! 무화과스콘, 라즈베리버터스콘, 초코쿠키, 초코스콘 존맛탱인데..? 나중에 오면 집에 사와야짓/4\n",
            "케이크 커피 다 맛있음 자리도 널찍/5\n",
            "오레오케이크...와.. 사람 많을 줄 알았는데 금요일 저녁인데도 거의 없었고 작업/스터디 공간으로도 이용되는 것 같아요. 노래 크기도 적당해서 얘기하기도 편하구요. 3층까지 있어서 널찍하고 테이블 간 거리가 멀어서 좋았어요./5\n",
            "케이크랑 스콘이 맛있당 근데 자주 가게되진않음/4\n",
            "/3\n",
            "이 정도면 괜찮음 스콘도 맛있고/4\n",
            "커피 맛있고, 디저트도 정말 맛있음!! 3층까지 있어서 미팅 잡기도 좋았어요~!/5\n",
            "굿 맛잇음/5\n",
            "다른카페 문닫고 3-4명인데 갈 곳 마땅치 않으면 = 발루토.  오픈때부터 다녔는데ㅠㅠ 좀 변한것 같아 아쉬워요.. 메뉴는 다양해져서 골라먹을 수 있다는 장점은 있습니다. 주문하고 좀 오래 기다리셔야해요 여유를 가지고 천천히~ /3\n",
            "케익 맛있었어요/5\n",
            "개인적으로 대학동 제일 맛있는 디저트/4\n",
            "3.2점밖에 안된다? 절대 말 안됨 ㅋㅋ ㅜㅜ 빵 너무 맛있어요/5\n",
            "2020_5/26 저녁 11시 카운터 내림머리 남자알바 분 참 인상 깊으시네요.   처음 뵙는 학생 분이었는데 존대 주문에 끝까지 반존대로 일관하시고,  늦은 처리에 배려로 천천히 하시라고 카드 맡겨두고 올라갔다 왔더니  잘못 포장에도 사과 한말씀 커녕 본인 표정 때문에 참 언짢고 불쾌하네요.  환불 괜찮으니 그냥 주시라 했는데도 음...ㅋㅋ그냥 황당함에 .../1\n",
            "오픈 때부터 자주 가던 곳인데 맛은 그대로이고 직원태도는 많이 변했네요. 사장님 부부가 많이 바쁘신가봐요. 시기가 시기인만큼 마스크착용 좀 하세요. 5월10일 오후 4시에 커피 만드시던 남자분 마스크도 안 쓰고 커피 만들면서 헛기침 계속 하던데요./1\n",
            "/5\n",
            "자주 오는 곳인데 평도 많이 올라오는데 개선 안되는 걸 보면 절이 싫으면 중이 떠나야하는 건가봐요ㅠㅠ  0. 커피/케이크/스콘 맛은 인근 카페들 중 상당히 좋은 편  1. 직원들 마스크 없고, 코로나때문이 아니더라도 웃고 활발히 얘기하는 분위기라면 음식에 침 안튀게 조리용 마스크라도 해야할 것 같음  2. 주문한 메뉴 나와서 컵 내려놓다가 테이블에 음료 흘.../2\n",
            "대왕쿠키 진짜 제 인생쿠키입니다 진짜 맛있어요/5\n",
            "스콘 케익은 맛남, 직원 응대 진짜 별로, 특히 여자분 맨날 인상 안 좋으심, 늦게 나오는 건 그렇다 쳐도 알바생들끼리 계속 얘기하고 웃고 노느라 손님 밀려도 신경도 안 씀, 스콘 케익 맛나니까 어쩔 수 없이 감./1\n",
            "커피는 맛있는데 직원들 태도가 진짜 별로/2\n",
            "/1\n",
            "분위기 좋고 커피 맛있는데 이층에서 화잘실 냄새 나요...../3\n",
            "분위기 좋은 카페/4\n",
            "스콘: 아침할인이 있었을 때가 좋았다. 하지만 카카오현미는 언제 먹어도 최고...! 일하는 분들이 무심해보이는 건 개선해야 할 듯  커피가 비싸지 않으면서 맛이 괜찮고 인테리어는 층마다 같지 않으면서도 통일감이 있음 공들인 티가 남. 공간도 널찍하니... 무엇보다 불편한 테이블과 의자 아니라서 머무르기 좋음 케익류 무난. 사실 양 생각하면 혜자 특색있는 기.../4\n",
            "기본인 스콘이 뭔 밥솥ㅇ 찐마냥 떡임 맛도 없고 종업원들은 자기네들끼리 친목 오져서 주문 들어와도 멀꿍멀뚱 왜 인기많은지 모르겠/1\n",
            "커피도 디저트도 맛있고 좋은데 직원분들이 좀 불성실해요. 주문이 별로 밀리지않았는데도 설렁설렁해서 모든 제품이 다 너무 늦게 나와요, 주문 밀려있으면 무조건 그냥 나가게 된다는.../3\n",
            "제일 기본인 아메리카노도 맛있고, 그 외 여러 음료들이 전부 괜찮습니다. 케잌 위주로 먹었을 때도 실패한 적 없고, 주변 사람들한테 여러번 추천했는데 전부 맛있었단 평이 많았습니다. /5\n",
            "레드벨벳 케이크 맛있어요 딸기라떼도 적당히달고 좋아요! 근대 카카오라떼는 쓴편이고 ..?요건 별로에요!/5\n",
            "두가지 블랜딩과 한가지 싱글오리진 커피를 드실 수 있습니다. (과테말라 인도네시아 브라질)내츄럴 블랜딩과 싱글오리진(케냐)는 라떼에 좋고 쿠키와 잘 어울립니다. 워시드 블랜딩은 아메리카노(핫,아이스) 라떼(아이스)에 드시기 좋습니다~ 싱글오리진 커피 에스프레소로 도전해 보세요! 커플분들이 많이 찾으시는 공간이며, 혼자가셔도 괜찮습니다 :) 다만 특정시간에 .../5\n",
            "겨울엔 난방비 절약하시더니 여름엔 냉방비 절약하시네요.. 공간별로 냉방시작시간을 다르게 하는 건 충분히 이해되는데, 이 폭염에도 에어컨을 너무 인색하게 틀어주심.. 온도 낮춰딜라고 부탁해도 효과가 미미함. 재미있는 건 카운터가 있는 1층만 핵시원함. 대학동에서야 대체할만한 카페가 없으니 갑니다만, 짠내나는 인상은 쉬이 가시질 않을 듯./1\n",
            "사진은 순서대로 말차케이크와 아이스 라떼, 오렌지 페코 홍차와 버터 밀크 스콘, 쑥스럽게 케이크와 아메리카노 각종 케이크가 있지만 주기적으로 생각나는 맛은 쑥스럽게인 듯. 홍차는 믈레즈나. 모히또에이드 맛있음. 그런데 카페 다른 동네에 확장하더니 베이커리류를 줄였나 봄? 흠.. 잘 안 가게 된다./4\n",
            "녹차쉬폰케익이었나 진짜 맛있었다. 음료도 괜찮았음. 서비스 안좋다는 평이 많았지만 난 전혀 못느꼈다. 일반적이었는데.. 카페가 널널하고 커서 좋았다/5\n",
            "커피 맛있고 베이커리 괜찮고 분위기 좋은데 젊은 직원들 너무 불친절해요.  직원 서비스교육에 신경쫌 쓰셔야할듯./5\n",
            "/5\n",
            "자주 갔는데요. 알바생 관리가 안되는 것 같네요. 주문한다고 말했는데 직원이 3명이 있어서 각자 본인일들만 하시고..듣는 척도 안함. 손님이 별로 없는 상황이었어요. 모은 쿠폰만 다 쓰고 안가려구요./1\n",
            "/4\n",
            "I love it! Their single origin coffee choice is amazing. Cool interior, nice music. You can get your coffee in a glass instead of a plastic cup  굿굿 /5\n",
            "[가격] 괜찮은 가격대 3000~5000원대  동네물가가 저렴해서 주위에 여기보다 저렴한카페에 비교해 비싸다구 느낄수 있겠다(디저트같은..)그래도 이정도면 저렴한편!  [분위기] 인테리어 자체가 삐까뻔적한곳이 아닌 정리되지않았지만 마치 일부러 마감처리 하지 않은듯한 분위기 이런 분위기도 괜찮게 꾸며놈(부셔진 벽을 통한 계단 등등) 젊은 사람들이 많이 온다..../4\n",
            "/2\n",
            "/3\n",
            "/3\n",
            "자주 가는데요. 요즘들어 2층 와이파이가 잘 안 터지고, 라떼 맛이 그때그때 좀 다르네요. 난방도 좀 인색하고.. 창틀에 걸린 그림 중 쌩뚱맞은 건 과감히 치우는 것도 좋을 듯. 그래도 고시촌 카페 중 그나마 괜춘./3\n",
            "no review in crawling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NoSuchElementException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4cc13aab9081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-4cc13aab9081>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#####\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-4cc13aab9081>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(place)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# 검색된 첫 페이지 장소 목록 크롤링하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mcrawling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0msearch_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-4cc13aab9081>\u001b[0m in \u001b[0;36mcrawling\u001b[0;34m(place, place_lists)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mdetail_page_xpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'//*[@id=\"info.search.place.list\"]/li['\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m']/div[5]/div[4]/a[1]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail_page_xpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 상세정보 탭으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"info.search.place.list\"]/li[4]/div[5]/div[4]/a[1]\"}\n  (Session info: headless chrome=91.0.4472.101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOMPfzZj-51R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cf4b627-8322-4cfd-9a1e-f4e9fcb090a2"
      },
      "source": [
        "import os\n",
        "from time import sleep\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.common.exceptions import ElementNotInteractableException\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "##############################################################  ############\n",
        "##################### variable related selenium ##########################\n",
        "##########################################################################\n",
        "# options = webdriver.ChromeOptions()\n",
        "# 크롬창(웹드라이버) 열기\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "# 구글 지도 접속하기\n",
        "driver.get(\"https://www.google.com/maps/\")\n",
        "\n",
        "# 검색창에 \"카페\" 입력하기\n",
        "searchbox = driver.find_element_by_css_selector(\"input#searchboxinput\")\n",
        "searchbox.send_keys(\"카페\")\n",
        "\n",
        "# 검색버튼 누르기\n",
        "searchbutton = driver.find_element_by_css_selector(\"button#searchbox-searchbutton\")\n",
        "searchbutton.click()\n",
        "\n",
        "def main():\n",
        "    global driver, load_wb, review_num\n",
        "\n",
        "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
        "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
        "\n",
        "    # 검색할 목록\n",
        "    place_infos = ['신림 카페']\n",
        "\n",
        "    for i, place in enumerate(place_infos):\n",
        "        # delay\n",
        "        if i % 4 == 0 and i != 0:\n",
        "            sleep(5)\n",
        "        print(\"#####\", i)\n",
        "        search(place)\n",
        "\n",
        "    driver.quit()\n",
        "    print(\"finish\")\n",
        "\n",
        "\n",
        "def search(place):\n",
        "    global driver\n",
        "\n",
        "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
        "    search_area.send_keys(place)  # 검색어 입력\n",
        "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
        "    sleep(1)\n",
        "\n",
        "    # 검색된 정보가 있는 경우에만 탐색\n",
        "    # 1번 페이지 place list 읽기\n",
        "    html = driver.page_source\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
        "\n",
        "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
        "    crawling(place, place_lists)\n",
        "    search_area.clear()\n",
        "\n",
        "    # 우선 더보기 클릭해서 2페이지\n",
        "    try:\n",
        "        driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
        "        sleep(1)\n",
        "\n",
        "        # 2~ 5페이지 읽기\n",
        "        for i in range(2, 6):\n",
        "            # 페이지 넘기기\n",
        "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
        "            driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
        "            sleep(1)\n",
        "\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
        "\n",
        "            crawling(place, place_lists)\n",
        "\n",
        "    except ElementNotInteractableException:\n",
        "        print('not found')\n",
        "    finally:\n",
        "        search_area.clear()\n",
        "\n",
        "\n",
        "def crawling(place, place_lists):\n",
        "    \"\"\"\n",
        "    페이지 목록을 받아서 크롤링 하는 함수\n",
        "    :param place: 리뷰 정보 찾을 장소이름\n",
        "    \"\"\"\n",
        "\n",
        "    while_flag = False\n",
        "    for i, place in enumerate(place_lists):\n",
        "        # 광고에 따라서 index 조정해야함\n",
        "        #if i >= 3:\n",
        "         #   i += 1\n",
        "\n",
        "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
        "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
        "\n",
        "        detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
        "        driver.find_element_by_xpath(detail_page_xpath).send_keys(Keys.ENTER)\n",
        "        driver.switch_to.window(driver.window_handles[-1])  # 상세정보 탭으로 변환\n",
        "        sleep(1)\n",
        "\n",
        "        print('####', place_name)\n",
        "\n",
        "        # 첫 페이지\n",
        "        extract_review(place_name)\n",
        "\n",
        "        # 2-5 페이지\n",
        "        idx = 3\n",
        "        try:\n",
        "            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
        "            for i in range(page_num-1):\n",
        "                # css selector를 이용해 페이지 버튼 누르기\n",
        "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
        "                sleep(1)\n",
        "                extract_review(place_name)\n",
        "                idx += 1\n",
        "            driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 5페이지가 넘는 경우 다음 버튼 누르기\n",
        "            sleep(1)\n",
        "            extract_review(place_name) # 리뷰 추출\n",
        "        except (NoSuchElementException, ElementNotInteractableException):\n",
        "            print(\"no review in crawling\")\n",
        "\n",
        "        # 그 이후 페이지\n",
        "        while True:\n",
        "            idx = 4\n",
        "            try:\n",
        "                page_num = len(driver.find_elements_by_class_name('link_page'))\n",
        "                for i in range(page_num-1):\n",
        "                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
        "                    sleep(1)\n",
        "                    extract_review(place_name)\n",
        "                    idx += 1\n",
        "                driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 10페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
        "                sleep(1)\n",
        "                extract_review(place_name) # 리뷰 추출\n",
        "            except (NoSuchElementException, ElementNotInteractableException):\n",
        "                print(\"no review in crawling\")\n",
        "                break\n",
        "\n",
        "        driver.close()\n",
        "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
        "\n",
        "\n",
        "def extract_review(place_name):\n",
        "    global driver\n",
        "\n",
        "    ret = True\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # 첫 페이지 리뷰 목록 찾기\n",
        "    review_lists = soup.select('.list_evaluation > li')\n",
        "\n",
        "    # 리뷰가 있는 경우\n",
        "    if len(review_lists) != 0:\n",
        "        for i, review in enumerate(review_lists):\n",
        "            comment = review.select('.txt_comment > span') # 리뷰\n",
        "            rating = review.select('.grade_star > em') # 별점\n",
        "            val = ''\n",
        "            if len(comment) != 0:\n",
        "                if len(rating) != 0:\n",
        "                    val = comment[0].text + '/' + rating[0].text.replace('점', '')\n",
        "                else:\n",
        "                    val = comment[0].text + '/0'\n",
        "                print(val)\n",
        "\n",
        "    else:\n",
        "        print('no review in extract')\n",
        "        ret = False\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### 0\n",
            "#### 디자이너리카페\n",
            "no review in extract\n",
            "환하고 예쁨. ㅁ자형 구조라 가운데 트인 부분이 기분을 상쾌하게 해줌. 커피맛은 잘 모르므로 이 부분에 대한 언급은 패스. 앉아서 노트북하고 공부하기 좋은 테이블, 의자였음. 재방문의사 있음./5\n",
            "매장 자체는 예쁜데 맛은 없어서 항상 재방문이 망설여짐/3\n",
            "넘 예쁘고 음료, 디저트도 맛나용/5\n",
            "장소가 주는 힙함. 아늑함. /4\n",
            "루프탑 풍경도 괜찮고 모던 인테리어치고 편안한 느낌. 햇빛이 잘 드는 지 화이트톤이라 그런건지 구름낀 날씨에도 눈부시고 맑아서 책 보기에 좋았음 친구들이랑 수다떨러 오고 싶은 곳. 근데 커피가 진짜 맛없음/3\n",
            "주말에 자리 진짜 없음.. 햇볕 강한 날 가면 직사광선에 온 몸 타는 기분을 느낄 수 있음 그리고 루프탑과 연결된 중정형?? 이라 그런지 더 시끄러운 것 같아어요. 도떼기시장 저리가라/3\n",
            "이쁜데 맛은 없음/3\n",
            "시설이랑 이런건 좋은데... 서비스는 조금 아쉽네요~! 조금더 좋게 해주셨으면 좋겠어요~/4\n",
            "/2\n",
            "/5\n",
            "분위기에 취한다/4\n",
            "자리값 커피는 모르겠다/3\n",
            "이 동네에 이런 카페가...? 와우! 믿기지않는다! 깔끔하고 넓은 내부, 독특한 인테리어에 루프탑까지 완벽- 다만, 커피맛이 좀 별로다. 여기 올거면 아아메 먹던지 차라리 병음료 추천 /4\n",
            "음료 진짜 맛없어요 자리 없는건 그러려니 하는데 시럽덩어리에 물 푼 맛임/1\n",
            "/4\n",
            "카페 인테리어는 정말 이쁘고 쾌적해서 자주오고 싶을것 같아요   그런데 커피양이 좀 적은것 같은건 기분탓이려나요 ㅠㅠ/3\n",
            "아메리카노 시켰는데 원두 다타서 두입먹고 다버림 뭐임이거 탄맛 에바잖어 ㅇ이거 다먹으면 암걸릴듯/1\n",
            "커피맛은 평범? 불만갖거나 감탄할 맛이 아니었고 디저트류도 지극히 평범 케잌은 떼오는것 같고 토스트는 빵 생크림 사과잼 모두가 평범합니다 의자도 딱딱하고 불편하고요 그렇지만 이곳을 먹으러 오는 곳이 아니라 널찍한 공간을 즐기러 오는 곳으로 생각한다면 신림에서 가장 만족스러운 공간일것입니다!/4\n",
            "카페는 예쁘지만 찾아가는 길 멀고 의자 불편함 재방문의사 없음/2\n",
            "신림에서 제일 힙한곳/4\n",
            "카페는 예쁜데 커피는 그냥그러네요. 디저트류는 비싼것같아요. /3\n",
            "의자 불편, 커피 적당/3\n",
            "영업시간을 고정할 필요가 있음 몇명이 헛걸음을 하는지 모든 사람들이 인스타만 들여다보고 살지는 않음/2\n",
            "엄청 힙한 가게가 신림에 생겼네요!/5\n",
            "새로 생긴지 얼마 안된 깔끔하고 예쁜 카페, 창이 크게 나서 햇빛도 잘 들고 커피도 맛있어요! 직원분들도 친절하네요 ㅎㅎㅎ/5\n",
            "이쁘고 커피맛도 좋고!! 사장님도 친절해여! /5\n",
            "no review in crawling\n",
            "#### 사생활\n",
            "no review in extract\n",
            "가보고 싶었던 만큼 맛있게 잘 먹고 왔으나 직원분 초큼 ㅎㅎ/4\n",
            "/5\n",
            "낮에 가는게 분위기 더좋아요 인테리어이쁨/5\n",
            "최악 불친절하고 시끄럽고 노래선곡 센스없음 음식도 가격대비 너무 비쌈 그나마 양이 적어서 맛없는거 겨우 먹고 도망치둣 나옴 /1\n",
            "파스타도 맛있고 돈가스도 맛있고 크로플까지 맛있음! 여기는 주기적으로 먹음..ㅠㅠ/5\n",
            "시골오일파스타가 정말 맛있어요 :)/5\n",
            "/4\n",
            "너무 추워서 꼬냑 한잔 마시러 들어갔는데 잔을 데워줬던 감성있는곳  /5\n",
            "메뉴에 없는 칵테일 주문했는데 세상제일 맛있었어요.. ^^b/5\n",
            "Amazing. Thank you/5\n",
            "양도 별로고 맛도 없고 원수 친구가 간다해도 말릴거같아요. 그리고 컵이 약간 지저분?해서 기분나빴어요/1\n",
            "세트로 시켰는데 음식이랑 음료가 같이 안 나옴...  목메여서 물마시면서 먹음. 음식 2/3 다먹어가는 시점에 음료가 나오더라구요...  결국엔 테이크아웃으로 가져감...  근데 더 늦게 오고 더 늦게 주문한 팀이 음식 음료 모두 먼저 받더군요 /1\n",
            "미팅있어서 근처에 외근갔다가, 너무 지쳐서  달달한거 먹으면서 위로받고 싶었다.   검색하다가 우연히 얻어 걸린 곳!  지하에 있는데 반층 내려가니 까페의 아이덴티티를 보여주듯 아기작한 입구가 보였다.   지하 내부는 쾌적했고, 조명 은은하고 깔끔.  가운데 큰 테이블이 있어서 노마드로 작업하기도 편해보였음.   곧 저녁 시간이라서 완전 배부른건 안되고...../5\n",
            "주택 지하1층에 있는 밥집 겸 카페 겸 술집. 분위기 좋습니다. 손님들 구성에 따라 너무 시끄럽다는 후기도 있었는데 제가 갔을때는 딱히 그렇지는 않았습니다. 읽을 만한 책과 잡지들이 있어 혼밥하고 쉬기에도 적합합니다. 주문한 메뉴는 닭가슴살 보울+음료 선택(아메리카노) 12,000원입니다. 생각보다 푸짐하게 나왔네요.  + 재방문 평일 저녁 늦게 갔는데 거.../5\n",
            "신대방역에 내리는 유일한 이유/5\n",
            "직원 분 짱 친절하시구 분위기도 맘에 들어요 낮에는 카페라던데 낮에도 가 보고 싶네요/5\n",
            "스카치에그랑 카레 정말 맛있는데, 고객수에 비해 일하는 사람 너무 적어서 메뉴 나오는데 한참걸리고 제가 방문한 날은 안되는 음료도 너무많았음. 장사가 잘되면 잘되는 만큼 사람을 더 고용하시는게 맞지않을까요/4\n",
            "이전에 조용하고 좋았는데 분위기가 바뀌고 나서  지금은 혼자서 노트북 영화보기에도 노래소리가 너무 커요 ㅠㅠㅠ 집중해서 노트북으로 작업하러 가시는분이나 가볍게 영상보러 가기에는 비추입니다 ㅠㅠ  분위기랑 노래는 좋으니 얘기나눌 분들은 가보셔도 좋을 것 같아요! /3\n",
            "커피 맥주와 함께 책을 읽을 수 있는 곳 조용한 분위기와 시그니처 음료, 음식들 정말 맛있어요 :) 가게가 구석구석 골목에 있어 찾기 힘들지만 가게에 들어가는 순간 다른 세계에 온 것 같은 느낌이 듭니다/5\n",
            "혼자만의 시간 갖기에 최적인 카페! 분위기 좋고 심지어 아메리카노도 아주 만족스러웠어요! 프릳츠 원두 사용하는거 같더라구요~ 다른 음료는 안먹어봐서 모르겠네요..ㅎㅎ/5\n",
            "사장님들이 갈 때마다 기억해주시고 착하셔서 기분이 좋슴다 분위기도 넘 좋구 라떼랑 술이 진짜 맛있어여 흘러 나오는 노래도 분위기랑 넘 잘 맞고 읽을 수 있게 구비된 책들도 너무 좋고요 20대에서 30대분들이 좋아할 카페예욤/5\n",
            "책 읽기 너무 좋아요 차분한 팝 음악도 마음에 들었고 치킨 파니니도 맛있었어요 양도 많고요 힐링되네요/5\n",
            "노래 대박이구 다 맛있음 .. /5\n",
            "분위기 깡패 커피 원두 장난아니네요.. 블루보틀 싱글오리진 섬머블랜드 사서 코박죽 할정도로 좋아하는데, 프릳츠... 메모.../5\n",
            "/4\n",
            "기본인 커피가 맛이없었다. 내가 간 시간만 그런건지 음악소리가 커서 아쉬웠음 클래식음악이 딱 좋을거같은 분위기인데/3\n",
            "/5\n",
            "no review in crawling\n",
            "#### 발루토\n",
            "no review in extract\n",
            "위생이랑 서비스 별로라고 적었는데 댓글 삭제하시나보네요 찔리긴 한 모양입니다/1\n",
            "경악했어요...어린이날 다음인가 직원이 일회용컵 닦아서 말리는거 봤어요... 그거보고 그냥 매장나옴./1\n",
            "신림동의 명물. 인생카페./5\n",
            "/5\n",
            "/4\n",
            "맛있지만 서비스가 영^^.. 바로 앞 계단에서 우당탕탕 넘어졌는데도 눈길도 안줌ㅋㅋ 키오스크도 이것보단 친절할듯..  옆에 계신 손님이 오히려 걱정해주고..(그분께 무한한 감사를••)/2\n",
            "/5\n",
            "스콘 맛집 커피는 쏘쏘  /4\n",
            "스콘은 맛있는데 대왕쿠키 대왕실망 ㅠㅜ /3\n",
            "맛잇는뎅?? 딸기라떼 딸기맛 지대루고 별로안담 아이스쿠림말차라떼엿나 그것두 진함  초코케익도 진하고 맛이쪔  +) 210108 재방문!! 무화과스콘, 라즈베리버터스콘, 초코쿠키, 초코스콘 존맛탱인데..? 나중에 오면 집에 사와야짓/4\n",
            "케이크 커피 다 맛있음 자리도 널찍/5\n",
            "오레오케이크...와.. 사람 많을 줄 알았는데 금요일 저녁인데도 거의 없었고 작업/스터디 공간으로도 이용되는 것 같아요. 노래 크기도 적당해서 얘기하기도 편하구요. 3층까지 있어서 널찍하고 테이블 간 거리가 멀어서 좋았어요./5\n",
            "케이크랑 스콘이 맛있당 근데 자주 가게되진않음/4\n",
            "/3\n",
            "이 정도면 괜찮음 스콘도 맛있고/4\n",
            "커피 맛있고, 디저트도 정말 맛있음!! 3층까지 있어서 미팅 잡기도 좋았어요~!/5\n",
            "굿 맛잇음/5\n",
            "다른카페 문닫고 3-4명인데 갈 곳 마땅치 않으면 = 발루토.  오픈때부터 다녔는데ㅠㅠ 좀 변한것 같아 아쉬워요.. 메뉴는 다양해져서 골라먹을 수 있다는 장점은 있습니다. 주문하고 좀 오래 기다리셔야해요 여유를 가지고 천천히~ /3\n",
            "케익 맛있었어요/5\n",
            "개인적으로 대학동 제일 맛있는 디저트/4\n",
            "3.2점밖에 안된다? 절대 말 안됨 ㅋㅋ ㅜㅜ 빵 너무 맛있어요/5\n",
            "2020_5/26 저녁 11시 카운터 내림머리 남자알바 분 참 인상 깊으시네요.   처음 뵙는 학생 분이었는데 존대 주문에 끝까지 반존대로 일관하시고,  늦은 처리에 배려로 천천히 하시라고 카드 맡겨두고 올라갔다 왔더니  잘못 포장에도 사과 한말씀 커녕 본인 표정 때문에 참 언짢고 불쾌하네요.  환불 괜찮으니 그냥 주시라 했는데도 음...ㅋㅋ그냥 황당함에 .../1\n",
            "오픈 때부터 자주 가던 곳인데 맛은 그대로이고 직원태도는 많이 변했네요. 사장님 부부가 많이 바쁘신가봐요. 시기가 시기인만큼 마스크착용 좀 하세요. 5월10일 오후 4시에 커피 만드시던 남자분 마스크도 안 쓰고 커피 만들면서 헛기침 계속 하던데요./1\n",
            "/5\n",
            "자주 오는 곳인데 평도 많이 올라오는데 개선 안되는 걸 보면 절이 싫으면 중이 떠나야하는 건가봐요ㅠㅠ  0. 커피/케이크/스콘 맛은 인근 카페들 중 상당히 좋은 편  1. 직원들 마스크 없고, 코로나때문이 아니더라도 웃고 활발히 얘기하는 분위기라면 음식에 침 안튀게 조리용 마스크라도 해야할 것 같음  2. 주문한 메뉴 나와서 컵 내려놓다가 테이블에 음료 흘.../2\n",
            "대왕쿠키 진짜 제 인생쿠키입니다 진짜 맛있어요/5\n",
            "스콘 케익은 맛남, 직원 응대 진짜 별로, 특히 여자분 맨날 인상 안 좋으심, 늦게 나오는 건 그렇다 쳐도 알바생들끼리 계속 얘기하고 웃고 노느라 손님 밀려도 신경도 안 씀, 스콘 케익 맛나니까 어쩔 수 없이 감./1\n",
            "커피는 맛있는데 직원들 태도가 진짜 별로/2\n",
            "/1\n",
            "분위기 좋고 커피 맛있는데 이층에서 화잘실 냄새 나요...../3\n",
            "분위기 좋은 카페/4\n",
            "스콘: 아침할인이 있었을 때가 좋았다. 하지만 카카오현미는 언제 먹어도 최고...! 일하는 분들이 무심해보이는 건 개선해야 할 듯  커피가 비싸지 않으면서 맛이 괜찮고 인테리어는 층마다 같지 않으면서도 통일감이 있음 공들인 티가 남. 공간도 널찍하니... 무엇보다 불편한 테이블과 의자 아니라서 머무르기 좋음 케익류 무난. 사실 양 생각하면 혜자 특색있는 기.../4\n",
            "기본인 스콘이 뭔 밥솥ㅇ 찐마냥 떡임 맛도 없고 종업원들은 자기네들끼리 친목 오져서 주문 들어와도 멀꿍멀뚱 왜 인기많은지 모르겠/1\n",
            "커피도 디저트도 맛있고 좋은데 직원분들이 좀 불성실해요. 주문이 별로 밀리지않았는데도 설렁설렁해서 모든 제품이 다 너무 늦게 나와요, 주문 밀려있으면 무조건 그냥 나가게 된다는.../3\n",
            "제일 기본인 아메리카노도 맛있고, 그 외 여러 음료들이 전부 괜찮습니다. 케잌 위주로 먹었을 때도 실패한 적 없고, 주변 사람들한테 여러번 추천했는데 전부 맛있었단 평이 많았습니다. /5\n",
            "레드벨벳 케이크 맛있어요 딸기라떼도 적당히달고 좋아요! 근대 카카오라떼는 쓴편이고 ..?요건 별로에요!/5\n",
            "두가지 블랜딩과 한가지 싱글오리진 커피를 드실 수 있습니다. (과테말라 인도네시아 브라질)내츄럴 블랜딩과 싱글오리진(케냐)는 라떼에 좋고 쿠키와 잘 어울립니다. 워시드 블랜딩은 아메리카노(핫,아이스) 라떼(아이스)에 드시기 좋습니다~ 싱글오리진 커피 에스프레소로 도전해 보세요! 커플분들이 많이 찾으시는 공간이며, 혼자가셔도 괜찮습니다 :) 다만 특정시간에 .../5\n",
            "겨울엔 난방비 절약하시더니 여름엔 냉방비 절약하시네요.. 공간별로 냉방시작시간을 다르게 하는 건 충분히 이해되는데, 이 폭염에도 에어컨을 너무 인색하게 틀어주심.. 온도 낮춰딜라고 부탁해도 효과가 미미함. 재미있는 건 카운터가 있는 1층만 핵시원함. 대학동에서야 대체할만한 카페가 없으니 갑니다만, 짠내나는 인상은 쉬이 가시질 않을 듯./1\n",
            "사진은 순서대로 말차케이크와 아이스 라떼, 오렌지 페코 홍차와 버터 밀크 스콘, 쑥스럽게 케이크와 아메리카노 각종 케이크가 있지만 주기적으로 생각나는 맛은 쑥스럽게인 듯. 홍차는 믈레즈나. 모히또에이드 맛있음. 그런데 카페 다른 동네에 확장하더니 베이커리류를 줄였나 봄? 흠.. 잘 안 가게 된다./4\n",
            "녹차쉬폰케익이었나 진짜 맛있었다. 음료도 괜찮았음. 서비스 안좋다는 평이 많았지만 난 전혀 못느꼈다. 일반적이었는데.. 카페가 널널하고 커서 좋았다/5\n",
            "커피 맛있고 베이커리 괜찮고 분위기 좋은데 젊은 직원들 너무 불친절해요.  직원 서비스교육에 신경쫌 쓰셔야할듯./5\n",
            "/5\n",
            "자주 갔는데요. 알바생 관리가 안되는 것 같네요. 주문한다고 말했는데 직원이 3명이 있어서 각자 본인일들만 하시고..듣는 척도 안함. 손님이 별로 없는 상황이었어요. 모은 쿠폰만 다 쓰고 안가려구요./1\n",
            "/4\n",
            "I love it! Their single origin coffee choice is amazing. Cool interior, nice music. You can get your coffee in a glass instead of a plastic cup  굿굿 /5\n",
            "[가격] 괜찮은 가격대 3000~5000원대  동네물가가 저렴해서 주위에 여기보다 저렴한카페에 비교해 비싸다구 느낄수 있겠다(디저트같은..)그래도 이정도면 저렴한편!  [분위기] 인테리어 자체가 삐까뻔적한곳이 아닌 정리되지않았지만 마치 일부러 마감처리 하지 않은듯한 분위기 이런 분위기도 괜찮게 꾸며놈(부셔진 벽을 통한 계단 등등) 젊은 사람들이 많이 온다..../4\n",
            "/2\n",
            "/3\n",
            "/3\n",
            "자주 가는데요. 요즘들어 2층 와이파이가 잘 안 터지고, 라떼 맛이 그때그때 좀 다르네요. 난방도 좀 인색하고.. 창틀에 걸린 그림 중 쌩뚱맞은 건 과감히 치우는 것도 좋을 듯. 그래도 고시촌 카페 중 그나마 괜춘./3\n",
            "no review in crawling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NoSuchElementException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-47a04eb59c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-47a04eb59c60>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#####\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-47a04eb59c60>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(place)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# 검색된 첫 페이지 장소 목록 크롤링하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mcrawling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0msearch_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-47a04eb59c60>\u001b[0m in \u001b[0;36mcrawling\u001b[0;34m(place, place_lists)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mdetail_page_xpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'//*[@id=\"info.search.place.list\"]/li['\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m']/div[5]/div[4]/a[1]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail_page_xpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 상세정보 탭으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"info.search.place.list\"]/li[4]/div[5]/div[4]/a[1]\"}\n  (Session info: headless chrome=91.0.4472.101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUoHZYh__AvY",
        "outputId": "894c5f13-73fa-4469-8879-8348eeea1474"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json            #json import하기\n",
        "\n",
        "custom_header = {\n",
        "    'referer' : 'https://www.mangoplate.com/',\n",
        "    'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' }\n",
        "\n",
        "\n",
        "def get_restaurants(name) :\n",
        "    # 검색어 name이 들어왔을 때 검색 결과로 나타나는 식당들을 리스트에 담아 반환\n",
        "    \n",
        "    restuarant_list = []\n",
        "    \n",
        "    url = \"https://www.mangoplate.com/search/\" + name\n",
        "    \n",
        "    req = requests.get(url, headers = custom_header)\n",
        "    \n",
        "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "    \n",
        "    div = soup.find_all(\"div\",class_=\"info\")\n",
        "    \n",
        "    for re in div:\n",
        "        href = re.find(\"a\")['href']\n",
        "        title = re.find(\"h2\",class_=\"title\").get_text().replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
        "        restuarant_list.append([href,title])\n",
        "    \n",
        "    return restuarant_list\n",
        "    \n",
        "\n",
        "def main() :\n",
        "    name = input()\n",
        "    \n",
        "    restuarant_list = get_restaurants(name)\n",
        "    \n",
        "    print(restuarant_list)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "신림\n",
            "[['/restaurants/CbdJqQErxKsX', '빠사삭'], ['/restaurants/HoWked2wivcI', '중화요리팔공'], ['/restaurants/DuYISQu8X9', '서울갈비'], ['/restaurants/A3itRxUfBs', '춘천골참숯불닭갈비'], ['/restaurants/N6zsknPkBG5l', '동학'], ['/restaurants/P_0ncHUU8dv6', '소미당'], ['/restaurants/bgbs23fGlnTp', '고모네정육식당'], ['/restaurants/cWfAVTOfVdLb', '발루토'], ['/restaurants/hX0tke6m83', '아리차이'], ['/restaurants/pS4G5e128O', '오첨지'], ['/restaurants/IY4bulaHdHWG', '호텔 베라노 (휴업중)'], ['/restaurants/-EOPKh3PhlzZ', '구루메키친'], ['/restaurants/hdeyUljHK5o6', '막불감동'], ['/restaurants/-C1tadV-lA', '다케'], ['/restaurants/RW7UpZAkatp2', '신림동피자집'], ['/restaurants/s5pHAbDq_71M', '마뇨떡볶이'], ['/restaurants/0ApBtrtmrK', '호남집'], ['/restaurants/LyypO1XY8u', '나폴리네'], ['/restaurants/ZknfMajVJu', '가네샤'], ['/restaurants/W7QiHQonSe', '삼촌네']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEMoKcvX_a6O",
        "outputId": "357a9e8f-a5d4-4265-8f0f-7dea65a1ee60"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "카페\n",
            "[['/restaurants/Fy2Z5kqgi-7b', '따빠디또'], ['/restaurants/aeO_cG35KO', '패스트리부티크(서울신라호텔)'], ['/restaurants/vcheyGIxPJ9D', '껠끄쇼즈'], ['/restaurants/0my2hMzHlOYD', '당도'], ['/restaurants/8GRQ6g5tH8GB', '플랜트(2호점)'], ['/restaurants/RHt4Mtrl4zHp', '듁스커피'], ['/restaurants/-czYKuW77F_5', '파네트크루아상팩토리(광화문점)'], ['/restaurants/G91eF6EI4DPc', '스티키리키'], ['/restaurants/XRoMziImmYCC', '뉴욕택시디저트'], ['/restaurants/SaCJTvz6fPV8', '티크닉'], ['/restaurants/faOJKeUy00Yt', '쎄쎄종'], ['/restaurants/QxTRZZv3Dl', '삐아프'], ['/restaurants/dPMQH4nlqZXc', '콰이어트 크림티'], ['/restaurants/KyJsLQGd64', '브라운브레드(도곡점)'], ['/restaurants/8mhsdtiBg0', '아티장베이커스(한남점)'], ['/restaurants/WjvK9ngVzlAg', '성심당(대전컨벤션센터점)'], ['/restaurants/DM2xUaLRreaa', '꼼다비뛰드'], ['/restaurants/JlEOAtsbIt', '포비(디타워점)'], ['/restaurants/q-ImZ-DGRhlY', '소울브레드'], ['/restaurants/jbXHGq3QwY', '폴앤폴리나(광화문점)']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZHejdZ-54H7",
        "outputId": "c6a7be12-a9fb-4f96-c4c0-8b00ded03312"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json            #json import하기\n",
        "\n",
        "custom_header = {\n",
        "    'referer' : 'https://www.mangoplate.com/',\n",
        "    'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' }\n",
        "\n",
        "def get_reviews(code) :\n",
        "    comments = []\n",
        "    \n",
        "    url = f\"https://stage.mangoplate.com/api/v5{code}/reviews.json?language=kor&device_uuid=V3QHS15862342340433605ldDed&device_type=web&start_index=0&request_count=5&sort_by=2\"\n",
        "    req = requests.get(url, headers = custom_header)\n",
        "    \n",
        "    if req.status_code == requests.codes.ok:\n",
        "        result = []\n",
        "        print(\"접속 성공\")\n",
        "        reviews = json.loads(req.text)\n",
        "        for i in range(5):\n",
        "            result.append(reviews[i][\"comment\"][\"comment\"][:20].replace(\"\\n\",\"\"))\n",
        "    else:\n",
        "        print(\"Error code\")\n",
        "    # req에 데이터를 불러온 결과가 저장되어 있습니다.\n",
        "    # JSON으로 저장된 데이터에서 댓글을 추출하여 comments에 저장하고 반환하세요.\n",
        "    return result\n",
        "    \n",
        "    \n",
        "\n",
        "def get_restaurants(name) :\n",
        "    url = f\"https://www.mangoplate.com/search/{name}\"\n",
        "    req = requests.get(url, headers = custom_header)\n",
        "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "    \n",
        "    # soup에는 특정 키워드로 검색한 결과의 HTML이 담겨 있습니다.\n",
        "    result = []\n",
        "    \n",
        "    restaurants = soup.find_all(\"div\",class_=\"info\")\n",
        "    \n",
        "    # 특정 키워드와 관련된 음식점의 이름과 href를 튜플로 저장하고,\n",
        "    for res in restaurants:\n",
        "        name = res.find(\"h2\",class_=\"title\").get_text().replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
        "        a_tag = res.find(\"a\")[\"href\"]\n",
        "        result.append((name,a_tag))\n",
        "    # 이름과 href를 담은 튜플들이 담긴 리스트를 반환하세요.\n",
        "    return result\n",
        "    \n",
        "def main() :\n",
        "    name = input(\"검색어를 입력하세요 : \")\n",
        "    \n",
        "    restuarant_list = get_restaurants(name)\n",
        "    \n",
        "    for r in restuarant_list :\n",
        "        print(r[0])\n",
        "        print(get_reviews(r[1]))\n",
        "        print(\"=\"*30)\n",
        "        print(\"\\n\"*2)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "검색어를 입력하세요 : 신림\n",
            "빠사삭\n",
            "접속 성공\n",
            "['튀김 맛집.. 그동안 먹었던 튀김과는', '최고의튀김... 빠사삭으로 오리까지 ', '너무 너무 가고싶어서 벼르다가 오늘 ', '관악구 뜨기전에 녹두 맛집 다 뿌숴야', '떡볶이 먹을 때 튀김 안먹는 사람인데']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "중화요리팔공\n",
            "접속 성공\n",
            "['짜장면 볶음밥 탕수육 먹었는데 짜장 ', '짜장면과 볶음밥 주문짜장은 양파가', '부천에서 종종 가는 중식당에서 추천을', '맛있어요...', ' 짜장면이 6천원인데 간짜장이 따로 ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "서울갈비\n",
            "접속 성공\n",
            "['고기 특징없고 소스도 제품맛 물씬.', '우삼겹, 된장찌개, 공기밥신림의 ', '우삼겹맛집맛있다! 이미 양념이 ', '종종들리는 맛집딱 땡기는 달콤소스+', '맛있어요 ㅎㅎ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "춘천골참숯불닭갈비\n",
            "접속 성공\n",
            "['닭갈비 4/5 : 적당히 괜찮음. 부', '맛있게 잘 먹고왔어요. 막국수랑 된장', '불친절하고 맛별로', '간판없는 닭갈비집!! 동네에 사는 아', '토속적이고 깊은 맛 좋네요']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "동학\n",
            "접속 성공\n",
            "['아 컨셉최고...  여기 구조장난없네', '해물파전을 먹었는데 그냥 그랬어요. ', '영화에 나오는 지하세계인줄 알았다. ', '#냠냠여름하면 장마 장마하면 막걸', '막걸리랑 전을 입에도 안 대는 내가 ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "소미당\n",
            "접속 성공\n",
            "['일단 가게는 작다 6테이블 하나는 1', '연어덮밥이 말도 안되게 맛있는 곳', '믹스가츠동 7500 싼것같다. 맛있다', '맛있어요!!연어 엄청 신선하고, ', '사진은 사케동(9500원). 밥알은']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "고모네정육식당\n",
            "접속 성공\n",
            "['8천원의 행복이랄까.#고모네정육식', '생각없이 방문했는데 이 가격에 이 정', 'It’s totally worth t', '고모네 정육식당(고모네 육회맛집) ★', '육회비빔밥 1만원, 육회냉면 8천원 ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "발루토\n",
            "접속 성공\n",
            "['스콘의 기본기가 되어있는 집택배로도', '녹두 카페 추천해달라고 하면 할리스.', '스콘 맛집이라고 먼 길을 찾아가면 실', '넓은 테이블 간격,혼자 작업하기 편', '이번엔 발루토에서 밀크티를 마셔보았네']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "아리차이\n",
            "접속 성공\n",
            "['3.5/5전반적인 식사는 좋지만 주', '.', '삼선해물짬뽕 (10000)근래 먹었', '신림짬뽕 9000 삼선짜장 9000 ', '짬뽕 양이 많아 놀랐습니다짬뽕은 맛']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "오첨지\n",
            "접속 성공\n",
            "['오삼불고기로 주문. 향긋한 미나리. ', '맛집 불모지에 이런 곳이?낙삼불고기', '오삼불고기와 볶음밥 미나리가 많이 ', '헉 맛있음', '신림역 오첨지 ●●●●○볶음밥이 ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "호텔 베라노 (휴업중)\n",
            "접속 성공\n",
            "['다들 맛있게 먹길래 궁금해서 주문. ', '냠냠', '저 배달 최애 식당 바꼈잖아요. 호텔', '이사 와보니.. 여기 만한 곳이 없다', '배달음식에 감성 한 스푼,,✔️ ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "구루메키친\n",
            "접속 성공\n",
            "['오늘의 사시미 32pcs 신선하고 맛', '소문만 무성하게 들어 가보고싶었던 구', '깔끔하게 먹을 수 있어서 좋았어요', '맛잇엇읍니다', '구루메키친은 힙합이다.독특한 메뉴']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "막불감동\n",
            "접속 성공\n",
            "['#주님이먹여살려주시는인생 #막불감동', '신림동에 핫한 막국수집 메밀면발 너무', '내가 면 한 그릇을 안 남기고 다 먹', '이런데에 맛집이 있었나? 라는 의심이', '고기랑 냉면 먹고싶을 때 괜찮은 선택']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "다케\n",
            "접속 성공\n",
            "['좋은 재료와 깔끔한 구성으로 최고의 ', '서울 최고의 가성비 돈까스...솔직', '별점이 4점대라 기대하고 갔지만 가성', '사진은 이곳 대표메뉴인 히레카츠(안심', '히레 7500 로스 7000가성비']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "신림동피자집\n",
            "접속 성공\n",
            "['⭐⭐⭐⭐☆트러플 풍기 (16.0),', '평범 친절', '매우 깔끔한 인테리어에 고급스러운 분', '맛있다. 피자가아쉽다. 신림이', '지이인짜 2주간 닭가슴살, 고구마만 ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "마뇨떡볶이\n",
            "접속 성공\n",
            "['떡볶이보다 튀김 맛집튀김 중에서도 ', '두끼보다 알차고 맛있는 느낌돈까스도', '가성비 최고이고 허니도넛? 이라고하는', '미친가성비의 떡볶이!!성인 7,9 ', '#신림 #신림역 #마뇨떡볶이 (신림점']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "호남집\n",
            "접속 성공\n",
            "['다들 신림동 오면 순대볶음만 먹는데', '신림동 순대 타운에 가면 시간이 멈춘', '신림역 백순대타운 2층에 위치한 호남', '신입생 적응기 필수코스였던 신림순대티', '2인분에 15000원이라니. 게다가 ']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "나폴리네\n",
            "접속 성공\n",
            "['추천합니다 가성비 있는 이탈리아 음식', '오오!!이러면서 흥분하며 먹은메뉴는 ', '동네 음식점이 평일 점심에 웨이팅 있', '여긴 피자는 쏘쏘한데 파스타가 존맛탱', '가본 동네 화덕피자집들 중 제일 괜찮']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "가네샤\n",
            "접속 성공\n",
            "['가성비 좋은 녹두 인도커리! 배달해서', '#비건 #알루고비 #대존맛 (지난번과', '이날 방문한 곳은 조금 색다른 메뉴로', '그래도 신림 고시촌에 이정도면 낫밷', '향신료가 엄청센 인도커리집27000']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n",
            "삼촌네\n",
            "접속 성공\n",
            "['맛있고 친절해요.', '‘신림순대타운.. 마치 부평 던전같은', '#신림#백순대', '\"즉석떡볶이처럼 재밌고 맛있는 백순대', '가성비때문에 가는 곳!참기름에 절인']\n",
            "==============================\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "pbxFxVyIB-KV",
        "outputId": "75ca947d-690c-4953-d31d-ff175a4b7aec"
      },
      "source": [
        "import nowon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9ab874a6fd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnowon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nowon'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "3bV2cK6pA5nQ",
        "outputId": "6a97dcdb-bf4f-4679-82d7-255d089cfe0c"
      },
      "source": [
        "rating_list = []\n",
        "userid_list = []\n",
        "itemid_list = []\n",
        "timestamp_list = []\n",
        "comment_list = []\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "for k, row in nowon.iterrows():\n",
        "    url = nowon[\"kakao_url\"][k]\n",
        "    print(k,\"번째 크롤링 중 : \",nowon[\"상호명\"][k] , sep=\" \")\n",
        "    driver.get(url)\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "try:\n",
        "    rating_num = driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_sorting > a > span.color_b').text\n",
        "    rating_num = int(rating_num)\n",
        "    print(rating_num)\n",
        "    \n",
        "    # 별점 개수로 페이지를 넘기는 기준을 정하기로 함 / 한 페이지 당 최대 리뷰는 5개!\n",
        "    if rating_num > 0 and rating_num <= 5:\n",
        "        p_num = 1\n",
        "        print(p_num)\n",
        "        \n",
        "        time.sleep(2)\n",
        "        \n",
        "        html = driver.page_source\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "        review_lists = soup.select('.list_evaluation > li')\n",
        "        time.sleep(3)\n",
        "        for j in range(0, len(user)):\n",
        "            userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "            itemid_list.append(nowon[\"상호명\"][k])\n",
        "            timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "            time.sleep(3)\n",
        "        if len(review_lists) != 0:\n",
        "            for j, review in enumerate(review_lists):\n",
        "                comment = review.select('.txt_comment > span') \n",
        "                rating = review.select('.grade_star > em') # 별점\n",
        "                        \n",
        "                if len(comment) != 0:\n",
        "                    if len(rating) != 0 :\n",
        "                        comment_list.append(comment[0].text) \n",
        "                        rating_list.append(rating[0].text)\n",
        "                        time.sleep(3)\n",
        "                    else:\n",
        "                        comment_list.append(comment[0].text) \n",
        "                        rating_list.append('0')\n",
        "        else:\n",
        "            print('리뷰가 없습니다.')\n",
        "            time.sleep(3)    \n",
        "                \n",
        "                \n",
        "    # 2 페이지\n",
        "    elif rating_num >= 6 and rating_num <= 10:\n",
        "        html = driver.page_source\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "        review_lists = soup.select('.list_evaluation > li')\n",
        "        time.sleep(3)\n",
        "        for j in range(0, len(user)):\n",
        "            userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "            itemid_list.append(nowon[\"상호명\"][k])\n",
        "            timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "            time.sleep(3)\n",
        "        if len(review_lists) != 0:\n",
        "            for j, review in enumerate(review_lists):\n",
        "                comment = review.select('.txt_comment > span') \n",
        "                rating = review.select('.grade_star > em') # 별점\n",
        "                        \n",
        "                if len(comment) != 0:\n",
        "                    if len(rating) != 0:\n",
        "                        comment_list.append(comment[0].text) \n",
        "                        rating_list.append(rating[0].text)\n",
        "                        time.sleep(3)\n",
        "                    else:\n",
        "                        comment_list.append(comment[0].text) \n",
        "                        rating_list.append('0')\n",
        "        else:\n",
        "            print('리뷰가 없습니다.')\n",
        "            time.sleep(3)\n",
        "                \n",
        "        driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a').click()\n",
        "        time.sleep(2)\n",
        "        html = driver.page_source\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "        review_lists = soup.select('.list_evaluation > li')\n",
        "        time.sleep(3)\n",
        "        \n",
        "        for j in range(0, len(user)):\n",
        "            userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "            itemid_list.append(nowon[\"상호명\"][k])\n",
        "            timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "            time.sleep(3)\n",
        "        if len(review_lists) != 0:\n",
        "            for j, review in enumerate(review_lists):\n",
        "                comment = review.select('.txt_comment > span') \n",
        "                rating = review.select('.grade_star > em') # 별점\n",
        "                        \n",
        "                if len(comment) != 0:\n",
        "                    if len(rating) != 0:\n",
        "                        comment_list.append(comment[0].text) \n",
        "                        rating_list.append(rating[0].text)\n",
        "                        time.sleep(3)\n",
        "                    else:\n",
        "                        comment_list.append(comment[0].text) \n",
        "                        rating_list.append('0')\n",
        "        else:\n",
        "            print('리뷰가 없습니다.')\n",
        "            time.sleep(3)\n",
        "                \n",
        "    # 3페이지 이하         \n",
        "    elif rating_num >= 11 and rating_num <= 15:\n",
        "        p_num = 3\n",
        "        print(p_num)\n",
        "                \n",
        "        for i in range(1,p_num+1):\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "            review_lists = soup.select('.list_evaluation > li')\n",
        "            time.sleep(3)\n",
        "            for j in range(0, len(user)):\n",
        "                userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "                itemid_list.append(nowon[\"상호명\"][k])\n",
        "                timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "            \n",
        "        # 카카오맵의 개인 user별 url의 규칙은 다음과 같습니다.\n",
        "                time.sleep(3)\n",
        "            if len(review_lists) != 0:\n",
        "                for j, review in enumerate(review_lists):\n",
        "                    comment = review.select('.txt_comment > span') \n",
        "                    rating = review.selec('.grade_star > em')\n",
        "                        \n",
        "                    if len(comment) != 0:\n",
        "                        if len(rating) != 0:\n",
        "                            comment_list.append(comment[0].text) \n",
        "                            rating_list.append(rating[0].text)\n",
        "                            time.sleep(3)\n",
        "                        else:\n",
        "                            comment_list.append(comment[0].text) \n",
        "                            rating_list.append('0')\n",
        "            else:\n",
        "                print('리뷰가 없습니다.')\n",
        "            try: # 마지막 페이지 크롤링을 위해서 try사용\n",
        "                driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(i) +']').click() # 페이지별로 실행하고\n",
        "            except:\n",
        "                continue\n",
        "            time.sleep(2)\n",
        "            \n",
        "            \n",
        "    # 4페이지     \n",
        "    elif rating_num >= 16 and rating_num <= 20:\n",
        "        p_num = 4\n",
        "        print(p_num)\n",
        "        \n",
        "        for i in range(1,p_num+1):\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "            review_lists = soup.select('.list_evaluation > li')\n",
        "            time.sleep(2)\n",
        "            for j in range(0, len(user)):\n",
        "                userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "                itemid_list.append(nowon[\"상호명\"][k])\n",
        "                timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "        # 카카오맵의 개인 user별 url의 규칙은 다음과 같습니다.\n",
        "            time.sleep(2)\n",
        "            \n",
        "            if len(review_lists) != 0:\n",
        "                for j, review in enumerate(review_lists):\n",
        "                    comment = review.select('.txt_comment > span') \n",
        "                    rating = review.select('.grade_star > em')\n",
        "                        \n",
        "                    if len(comment) != 0:\n",
        "                        if len(rating) != 0:\n",
        "                            comment_list.append(comment[0].text) \n",
        "                            rating_list.append(rating[0].text)\n",
        "                            time.sleep(3)\n",
        "                        else:\n",
        "                            comment_list.append(comment[0].text) \n",
        "                            rating_list.append('0')\n",
        "            else:\n",
        "                print('리뷰가 없습니다.')\n",
        "            try: # 마지막 페이지 크롤링을 위해서 try사용\n",
        "                driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(i) +']').click() # 페이지별로 실행하고\n",
        "            except:\n",
        "                continue\n",
        "            time.sleep(2)\n",
        "        \n",
        "                    \n",
        "        \n",
        "                    \n",
        "    # 5페이지 이상           \n",
        "    elif rating_num >= 21:\n",
        "        p_num = int(math.ceil(rating_num / 5))\n",
        "        print(p_num)\n",
        "        \n",
        "        for i in range(1, 7):\n",
        "            p_num -= 1\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "            review_lists = soup.select('.list_evaluation > li')\n",
        "            time.sleep(2)\n",
        "            for j in range(0, len(user)):\n",
        "                userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "                itemid_list.append(nowon[\"상호명\"][k])\n",
        "                timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "            # 카카오맵의 개인 user별 url의 규칙은 다음과 같습니다.\n",
        "                time.sleep(3)\n",
        "            \n",
        "            \n",
        "                # 리뷰 리스트가 0이 아니면\n",
        "            if len(review_lists) != 0:\n",
        "                for j, review in enumerate(review_lists):\n",
        "                    comment = review.select('.txt_comment > span') \n",
        "                    rating = review.select('.grade_star > em') # 별점\n",
        "                        \n",
        "                    if len(comment) != 0:\n",
        "                        if len(rating) != 0:\n",
        "                            comment_list.append(comment[0].text) \n",
        "                            rating_list.append(rating[0].text)\n",
        "                            time.sleep(3)\n",
        "                        else:\n",
        "                            comment_list.append(comment[0].text) \n",
        "                            rating_list.append('0')\n",
        "            else:\n",
        "                print('리뷰가 없습니다.')\n",
        "            try: # 마지막 페이지 크롤링을 위해서 try사용\n",
        "                driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(i) +']').click() # 페이지별로 실행하고\n",
        "            except:\n",
        "                continue\n",
        "            time.sleep(2)\n",
        "            print(p_num)\n",
        "            pp_num = p_num\n",
        "        while p_num <= pp_num and p_num >= 0:\n",
        "            for a in range(2,7) :\n",
        "                p_num -= 1\n",
        "                html = driver.page_source\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "                review_lists = soup.select('.list_evaluation > li')\n",
        "                user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
        "                time.sleep(2)\n",
        "                for j in range(0,len(user)):\n",
        "                    userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
        "                    itemid_list.append(nowon[\"상호명\"][k])\n",
        "                    timestamp_list.append(soup.select('.time_write')[j].text)\n",
        "                    time.sleep(2)\n",
        "                if len(review_lists) != 0:\n",
        "                    for i, review in enumerate(review_lists):\n",
        "                        comment = review.select('.txt_comment > span') # 리뷰\n",
        "                        rating = review.select('.grade_star > em') # 별점\n",
        "                        if len(comment) != 0:\n",
        "                            if len(rating) != 0:\n",
        "                                comment_list.append(comment[0].text) \n",
        "                                rating_list.append(rating[0].text)\n",
        "                                time.sleep(3)\n",
        "                            else:\n",
        "                                comment_list.append(comment[0].text) \n",
        "                                rating_list.append('0')\n",
        "                else:\n",
        "                    print('no review in extract')\n",
        "                try:\n",
        "                    driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(a) +']').click() # 페이지별로 실행하고\n",
        "                except:\n",
        "                    continue\n",
        "                time.sleep(3)    \n",
        "            \n",
        "            \n",
        "                            \n",
        "                                       \n",
        "\n",
        "except (NoSuchElementException, ElementNotInteractableException):\n",
        "    print(\"리뷰가 없습니다\")\n",
        "    continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c4f77f386326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnowon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnowon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kakao_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"번째 크롤링 중 : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnowon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"상호명\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nowon' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxzbzsgDB4VB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}